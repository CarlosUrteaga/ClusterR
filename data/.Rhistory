}
# root-mean-squared-error
rmse = function(y_true, y_pred) {
out = sqrt(mean((y_true - y_pred) ^ 2))
out
}
# mean-squared-error
mse = function(y_true, y_pred) {
out = mean((y_true - y_pred) ^ 2)
out
}
# simple prediction function
# DEFINITION : For a REGRESSION tree the predicted response for an observation is given by the mean response of the training observations
#              that belong to the same terminal node.
#              For a CLASSIFICATION tree we predict that each observation belongs to the most commonly occurring class of the training
#              observations in the region to which it belongs [ we are not only interested in the most commonly occurring class BUT also
#              in the class proportions among the training observations that fall into that region ]
tree_predict = function(obj, data) {
id_data = seq(1:dim(data)[1])
dat = cbind(ADD_ID = id_data, data)
dim_new = lapply(names(obj$preds_y), function(x) dim(subset(dat, eval(parse(text = x))))[1])
id_new = lapply(names(obj$preds_y), function(x) subset(dat, eval(parse(text = x)))[, 'ADD_ID'])
rep_resp = lapply(1:length(dim_new), function(x) rep(as.vector(obj$preds_y)[x], dim_new[[x]]))
out_df = data.frame(id = unlist(id_new), preds = unlist(rep_resp))
out_df = out_df[order(out_df[, 1], decreasing = FALSE), ]
return(out_df$preds)
}
# body of the random-forest-prediction-function
bod_rf_preds = function(data, sublist_pred_y) {
id_data = seq(1:dim(data)[1])
dat = cbind(ADD_ID = id_data, data)
dim_new = lapply(names(sublist_pred_y), function(x) dim(subset(dat, eval(parse(text = x))))[1])
id_new = lapply(names(sublist_pred_y), function(x) subset(dat, eval(parse(text = x)))[, 'ADD_ID'])
rep_resp = lapply(1:length(dim_new), function(x) rep(as.vector(sublist_pred_y)[x], dim_new[[x]]))
out_df = data.frame(id = unlist(id_new), preds = unlist(rep_resp))
#out_df = out_df[order(out_df[, 1], decreasing = FALSE), ]
out_df
}
# main random-forest-prediction-function
randomForest_predict = function(obj, data) {
suppressMessages(library(dplyr))
cat('it is advised to remove the response from the data before running the randomForest prediction function')
cat('\n')
# ensure that categorical variables will be converted to dummies [ exactly as in the tree_full_depth function]
prepr_categ = data
middl_categ_preds = func_categorical_preds(prepr_categ)
data = data[, -which(colnames(data) %in% middl_categ_preds$rem_predictors)]
data = cbind(data, do.call(cbind, middl_categ_preds$md_matr))
# iterate over all sublists
out = do.call(rbind, lapply(obj$pred_rules_y, function(x) bod_rf_preds(data, x)))
out = data.frame(out %>% group_by(id) %>% summarize(PREDS = mean(preds)))[, 2]
out
}
#############################################################################
# create exceptions for categorical variables
# adjust to classification
# calculate residuals, as I will use them also in boosting
# do extensive search in the split-function to improve error-rate
# parallelization OR more efficient functions to decrease runtime
# calculate R-squared for regression and misclassification rate for classification
# prediction-function for randomforest
# #fit = tree_full_depth(data, 'medv', max_node_size = 10, mtry = round(sqrt(dim(data)[2])), sample_size = 0.65, min_obs = 10)
# fit = tree_full_depth(data, 'medv', max_node_size = 8, mtry = 4, sample_size = 0.66, min_obs = 5, 'mse', proximity = TRUE, importance = NULL, permutation_function = NULL,
#
#                       maximize_permutation_metric = FALSE, smooth_predictions = TRUE, neighbors = 2)
#
# fit$train_error
# fit$OOB_error
# fit$prediction_rules
#
# # oob error
# pred_oob = tree_predict(fit, fit$OOB_data)
# rmse(fit$OOB_data$medv, pred_oob)
# mean((pred_oob - fit$OOB_data$medv) ^ 2)
#
# # train error
# pred_train = tree_predict(fit, data[fit$idx_train, ])
# rmse(data[fit$idx_train, 'medv'], pred_train)
###########################################################
# # first of all remove 'FALSE' from the sublists
#
# out_greater = lapply(fit$tree_out_greater$lst_rules_greater, function(x) unlist(x)[unlist(x) != 'FALSE'])
# out_greater = out_greater[unlist(lapply(out_greater, function(x) length(x) != 0))]
# out_less = lapply(fit$tree_out_less$lst_rules_less, function(x) unlist(x)[unlist(x) != 'FALSE'])
# out_less = out_less[unlist(lapply(out_less, function(x) length(x) != 0))]
#
#
# func_keep_rules(out_greater)
# func_keep_rules(out_less)
#
# as.vector(sapply(func_keep_rules(out_greater), function(x) dim(subset(data[fit$idx_train, ], eval(parse(text = x))))[1]))
# as.vector(sapply(func_keep_rules(out_less), function(x) dim(subset(data[fit$idx_train, ], eval(parse(text = x))))[1]))
#
# sum(sapply(func_keep_rules(out_greater), function(x) dim(subset(data[fit$idx_train, ], eval(parse(text = x))))[1])) + sum(sapply(func_keep_rules(out_less), function(x) dim(subset(data[fit$idx_train, ], eval(parse(text = x))))[1]))
# dim(data[fit$idx_train, ])
#
#
# sum(sapply(fit$prediction_rules, function(x) dim(subset(data[fit$idx_train, ], eval(parse(text = x))))[1]))
# dim(data[fit$idx_train, ])
#--------------------------------------------------------------------------------------------------------------------------------
#######################################################################################################################################################################################
#######################################################################################################################################################################################
# DON'T use mclapply() in the inner-loop as I use repeatedly lapply() in the tree_full_depth()-function [ use command-line-commands to split the trees to multiple cores (with shell-scripts) ]
# take a data.frame with 'id' and 'response-column' AND a list of data.frames with 'id' and 'prediction-values' and iteratively merge each of those data.frames with the response-data.frame
func_merge = function(response_df, list_of_dfs, merge_by) {
copy_resp_df = response_df
for (i in 1:length(list_of_dfs)) {
temp_df = list_of_dfs[[i]]
colnames(temp_df) = c(merge_by, paste0('preds_', i))
copy_resp_df = merge(copy_resp_df, temp_df, by = merge_by, all.x = TRUE)
}
copy_resp_df
}
#--------------------------------------------------------------------------------------------------------------------------------
# PROXIMITIES [ proximities can be used to 'replace missing values', 'locate outliers' , 'clustering' and 'investigate how the predictors separate the classes by locating 'prototypes' ]
# references : https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#prox,
#              http://www.listendata.com/2014/11/random-forest-with-r.html [ see proximity-similarity ]
#              http://www.alanfielding.co.uk/multivar/rf.htm [ Case Similarities , 'investigate how the predictors separate the classes by locating 'prototypes' ]
# calculate pairwise proximities for each tree separately >>>> then aggregate using dplyr over all trees
# the idea here is to first build all pairs of row.numbers of each sublist of each tree
library(gtools)            # use mixedsort from the gtools package
pairwise_func = function(sublist) {
comps <- t(combn(sublist, 2))
comps
}
# main function to calculate proximities
proximity_func = function(rf_lst) {
suppressMessages(library(dplyr))
out_each = list()
for (i in 1:length(rf_lst)) {
# use mixedsort() to first sort the character-numbers of the sublists, so that all sublists begin from smallest [ in that way I don't miss any pair-combinations ]
out_each[[i]] = data.frame(do.call(rbind, lapply(rf_lst[[i]], function(x) pairwise_func(mixedsort(x)))))
}
out = do.call(rbind, out_each)
out[, 1] = as.numeric(as.character(out[, 1]))
out[, 2] = as.numeric(as.character(out[, 2]))
colnames(out) = c('rows', 'columns')
# group matrix by two columns stackoverflow : http://stackoverflow.com/questions/21208801/group-by-multiple-columns-in-dplyr-using-string-vector-input
grp_cols <- names(out)                                                                 # Columns you want to group by
dots <- lapply(grp_cols, as.symbol)                                                    # Convert character vector to list of symbols
out1 = data.frame(out %>% group_by_(.dots = dots) %>% summarise(count = n()))          # Perform frequency counts
out1
}
#--------------------------------------------------------------------------------------------------------------------------------
# prototypes [ 'classCenter' function from the randomForest package ]    -- IT IS USED ONLY IN CLASSIFICATION
# COPY-PASTE FROM : http://www.inside-r.org/packages/cran/randomforest/docs/classCenter
# for prototypes see also : http://www.alanfielding.co.uk/multivar/rf.htm
classCenter <- function(x, label, prox, nNbr = min(table(label))-1) {
## nPrototype=rep(3, length(unique(label))), ...) {
label <- as.character(label)
clsLabel <- unique(label)
## Find the nearest nNbr neighbors of each case
## (including the case itself).
idx <- t(apply(prox, 1, order, decreasing=TRUE)[1:nNbr,])
## Find the class labels of the neighbors.
cls <- label[idx]
dim(cls) <- dim(idx)
## Count the number of neighbors in each class for each case.
ncls <- sapply(clsLabel, function(x) rowSums(cls == x))
## For each class, find the case(s) with most neighbors in that class.
clsMode <- max.col(t(ncls))
## Identify the neighbors of the class modes that are of the target class.
nbrList <- mapply(function(cls, m) idx[m,][label[idx[m,]] == cls],
clsLabel, clsMode, SIMPLIFY=FALSE)
## Get the X data for the neighbors of the class `modes'.
xdat <- t(sapply(nbrList, function(i) apply(x[i,,drop=FALSE], 2,
median)))
xdat
}
#--------------------------------------------------------------------------------------------------------------------------------
# MAIN FUNCTION FOR random-forest algorithm
# distributed = TRUE and no_slaves = 6 could be work in a cluster of computers with one OR
# more cores [ it can work if 'lopenblas' is not installed [SEE .theanorc file in hidden files linux ], my installation runs a fast openblas library (due to theano) which probably causes the crashes of the PC ]
# the other thing I can do is to use the batch-script, split the number of trees and then combine them, using save and load [ SEE folder "/home/lampros/Desktop/kaggle_gpu/FOLDER_SCRIPTS/INTRODUCTION_TO_STATISTICAL_LEARNING" ]
randomForest_func = function(n_trees = NULL, data, resp_name, max_node_size = NULL, mtry = NULL, sample_size = NULL, min_obs = NULL, eval_func_name = NULL, distributed = FALSE,
no_slaves = NULL, proximity = FALSE, importance = NULL, permutation_function = NULL, maximize_permutation_metric = FALSE, smooth_predictions = FALSE, neighbors = 2) {
if (is.null(n_trees)) n_trees = 5                                  # default number of trees == 5
if (is.null(no_slaves) && distributed == TRUE) no_slaves = 2       # default number of computer/core-slaves = 2
if (distributed == FALSE) {
cat('\n')
cat('sequential randomForest begins ...')
cat('\n')
all_trees_out = list()
cat('\n')
pb <- txtProgressBar(min = 0, max = n_trees, style = 3)
for (sgl_tree in 1:n_trees) {
all_trees_out[[sgl_tree]] = tree_full_depth(data, resp_name, max_node_size, mtry, sample_size, min_obs, eval_func_name, proximity, importance, permutation_function,
maximize_permutation_metric, smooth_predictions, neighbors)
setTxtProgressBar(pb, sgl_tree)
}
close(pb)}
else {
cat('\n')
cat('distributed randomForest begins ...')
cat('\n')
library(Rmpi) ; library(doMPI)
# https://github.com/berkeley-scf/parallelR-biostat-2015/blob/master/doMPIexample.R
cl <- startMPIcluster(count = no_slaves, verbose = FALSE)
registerDoMPI(cl) ; clusterSize(cl)
# http://stackoverflow.com/questions/6689937/r-problem-with-foreach-dopar-inside-function-called-by-optim  [ ABOUT the .export argument in foreach-function ]
all_trees_out <- foreach(i = 1:n_trees, .export = ls(envir = globalenv())) %dopar% {tree_full_depth(data, resp_name, max_node_size, mtry, sample_size, min_obs, eval_func_name, proximity,
importance, permutation_function, maximize_permutation_metric, smooth_predictions, neighbors)}
closeCluster(cl)
}
# predicted values for each rule of the trees aggregated for the random forest
preds_y_rf = lapply(all_trees_out, function(x) x$preds_y)
# list of the train-IDs-data.frames AND oob-IDs-data.frames
lst_train_IDs_preds = lapply(all_trees_out, function(x) x$train_IDs_preds)
lst_OOB_IDs_preds = lapply(all_trees_out, function(x) x$OOB_IDs_preds)
# list of the single-tree-train-error AND single-tree-OOB-error
lst_train_error = lapply(all_trees_out, function(x) x$train_error)
lst_OOB_error = lapply(all_trees_out, function(x) x$OOB_error)
# merge train-AND-oob-data.frames WITH THE ids-response-data.frame
id_resp = data.frame(ids = 1:dim(data)[1], y = data[, resp_name])
merg_train = func_merge(id_resp, lst_train_IDs_preds, 'ids')[, -c(1:2)]
merg_train = na.omit(cbind(id_resp, end_preds = rowMeans(merg_train, na.rm = TRUE)))[, -1]
merg_OOB = func_merge(id_resp, lst_OOB_IDs_preds, 'ids')[, -c(1:2)]
merg_OOB = na.omit(cbind(id_resp, end_preds = rowMeans(merg_OOB, na.rm = TRUE)))[, -1]
# calculate R^2 for both the train-fitted-values and the oob-values -- probably the oob-R^2 is more reliable [ I'll also need R^2 for the permutation-importance ]
r2_train = cor(merg_train$end_preds, merg_train$y) ^ 2
r2_oob = cor(merg_OOB$end_preds, merg_OOB$y) ^ 2
out_obj = list(AVG_single_tree_train_error = mean(unlist(lst_train_error)), AVG_single_tree_oob_error = mean(unlist(lst_OOB_error)), pred_rules_y = preds_y_rf)
if (proximity == TRUE) {
prox = lapply(all_trees_out, function(x) x$proximity_train)
start_prox_func = Sys.time()
out = proximity_func(prox)               # function to be used for data.frame of rows-columns-index and frequency of pairs
out$count = out$count/n_trees            # normalize frequency of pairs dividing by the number of trees
end_prox_func = Sys.time()
time_prox_func = end_prox_func - start_prox_func
cat('\n')
cat('It took', round(as.vector(time_prox_func), 3), attributes(time_prox_func)$units, 'to build the proximity data.frame with rows-columns-frequency', '\n')
cat('\n')
# THEN build a matrix with rownames : range of the row-dimensions of the data
#                          columns  : range of the row-dimensions of the data
start_iter_matr = Sys.time()
df_prox = matrix(rep(0, dim(data)[1] * dim(data)[1]), nrow = dim(data)[1], ncol = dim(data)[1])
# match rows AND columns and append probability -- iterative
for (x in 1:dim(out)[1]) {
df_prox[out[x, 1], out[x, 2]] = out[x, 3]        # here I take each row of the 'out' data.frame and I adjust the 'df_prox' matrix-rows-columns using the frequency-probs of the 'out' rows
}
end_iter_matr = Sys.time()
time_iter_matr = end_iter_matr - start_iter_matr
cat('It took', round(as.vector(time_iter_matr), 3), attributes(time_iter_matr)$units, 'to iteratively match rows-columns-normalized-proximity with empty matrix', '\n')
cat('\n')
out_obj[['proximity']] = df_prox
}
if (!is.null(importance) && importance == 'impurity') {
suppressMessages(library(dplyr))
temp_imp = do.call(rbind, lapply(all_trees_out, function(x) x$importance))
temp_imp = data.frame(temp_imp %>% group_by(split_predictor) %>% summarize(impurity = sum(decrease_node_purity), Frequency = length(split_predictor)))
temp_imp$impurity = temp_imp$impurity/n_trees
temp_imp = temp_imp[order(temp_imp[, 2], temp_imp[, 3], decreasing = TRUE),]
out_obj[['importance']] = temp_imp
}
if (!is.null(importance) && importance == 'permutation') {
cat('The permutation-importance is computed from the OOB-samples', '\n')
suppressMessages(library(dplyr))
temp_imp = do.call(rbind, lapply(all_trees_out, function(x) x$importance))
temp_imp = data.frame(temp_imp %>% group_by(predictor) %>% summarize(permutation = mean(permutation_importance)))
temp_imp = temp_imp[order(temp_imp[, 2], decreasing = TRUE),]
temp_imp$permutation = round(temp_imp$permutation, 4)
out_obj[['importance']] = temp_imp
}
#save(out_obj, file = "randomForest_object.RDATA")
cat('\n')
cat('summary                   :', '\n')
cat('number of trees used      :', '\t', n_trees, '\n')
cat('train-error               :', '\t', VALID_FUNC(eval_func_name, merg_train$y, merg_train$end_preds), '\n')
cat('out-of-bag-error          :', '\t', VALID_FUNC(eval_func_name, merg_OOB$y, merg_OOB$end_preds), '\n')
cat('train r-squared           :', '\t', r2_train, '\n')
cat('OOB r-squared             :', '\t', r2_oob, '\n')
return(out_obj)
}
dim(data)
rf = randomForest_func(n_trees = 50, data, 'medv', max_node_size = 9, mtry = 6, sample_size = 0.66, min_obs = 5, mse, distributed = FALSE, no_slaves = NULL,
proximity = FALSE, importance = NULL, permutation_function = NULL, maximize_permutation_metric = FALSE, smooth_predictions = FALSE, neighbors = 2)
rf = randomForest_func(n_trees = 50, data, 'medv', max_node_size = 6, mtry = 4, sample_size = 0.66, min_obs = 2, mse, distributed = FALSE, no_slaves = NULL,
proximity = FALSE, importance = NULL, permutation_function = NULL, maximize_permutation_metric = FALSE, smooth_predictions = FALSE, neighbors = 2)
rf = randomForest_func(n_trees = 30, data, 'medv', max_node_size = 6, mtry = 4, sample_size = 0.75, min_obs = 2, mse, distributed = FALSE, no_slaves = NULL,
proximity = FALSE, importance = NULL, permutation_function = NULL, maximize_permutation_metric = FALSE, smooth_predictions = FALSE, neighbors = 2)
library("OpenImageR", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.2")
detach("package:OpenImageR", unload=TRUE)
library("KernelKnn", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.2")
detach("package:KernelKnn", unload=TRUE)
library("AKNN", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.2")
detach("package:AKNN", unload=TRUE)
getwd()
load("TIBS_4set_RMA.RData")
load("v.RData")
load("v.rdata")
23.58117/5
data(iris)
dat = as.matrix(iris[, -ncol(iris)])
pm = cluster::pam(dat, 1)
pm$silinfo
pm$clusinfo
Rcpp::sourceCpp('Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/cluster_medoids.cpp')
Rcpp::sourceCpp('Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/cluster_medoids.cpp')
a = ClusterMedoids(dat, 3, 'euclidean', threads = 1, verbose = T, swap_phase = T, fuzzy = F)
str(a)
head(a$silhouette_matrix)
a = ClusterMedoids(dat, 1, 'euclidean', threads = 1, verbose = T, swap_phase = T, fuzzy = F)
str(a)
pm$silinfo
pm$clusinfo
colnames(cs) = c('clusters', 'number_obs', 'max_dissimilarity', 'average_dissimilarity', 'diameter', 'separation')
source('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/clustering_functions.R')
cm = Cluster_Medoids(dat, clusters = 1, distance_metric = 'euclidean', swap_phase = TRUE)
cm = Cluster_Medoids(dat, clusters = 1, distance_metric = 'euclidean', swap_phase = TRUE, verbose = T, fuzzy = T)
str(cm)
cm = Cluster_Medoids(dat, clusters = 2, distance_metric = 'euclidean', swap_phase = TRUE, verbose = T, fuzzy = T)
str(cm)
Rcpp::sourceCpp('Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/kmeans.cpp')
Rcpp::sourceCpp('Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/kmeans.cpp')
Rcpp::sourceCpp('Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/utils_rcpp.cpp')
setwd('/home/lampros/Desktop/kaggle_gpu/add_GITHUB/ClusterR/data')
load("dietary_survey_IBS.rda")
dietar = as.matrix(dietary_survey_IBS[, -ncol(dietary_survey_IBS)])
d = SCALE(dietar)
dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]
dat = center_scale(dat)
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'distortion_fK', plot_clusters = FALSE)
opt
dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]
X = center_scale(dat)
tmp_x = list(X)
Optimal_Clusters_KMeans(tmp_x, max_clusters = 10, criterion = 'distortion_fK', plot_clusters = FALSE)
class(dat)
res =  Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'distortion_fK', plot_clusters = FALSE)
res
nr_clust = 10
res =  Optimal_Clusters_KMeans(dat, max_clusters = nr_clust, criterion = 'distortion_fK', plot_clusters = FALSE)
source('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/clustering_functions.R')
res =  Optimal_Clusters_KMeans(dat, max_clusters = nr_clust, criterion = 'distortion_fK', plot_clusters = FALSE)
res
testthat::test_that("Optimal_Clusters_KMeans returns the correct output if the input is a data frame ", {
nr_clust = 10
res =  Optimal_Clusters_KMeans(dat, max_clusters = nr_clust, criterion = 'distortion_fK', plot_clusters = FALSE)
testthat::expect_true( length(res) == nr_clust && class(res) == "k-means clustering"  )
})
tmp_m = data.frame(1)
Optimal_Clusters_KMeans(X, max_clusters = tmp_m, criterion = 'distortion_fK', plot_clusters = FALSE)
source('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/clustering_functions.R')
Optimal_Clusters_KMeans(X, max_clusters = tmp_m, criterion = 'distortion_fK', plot_clusters = FALSE)
tmp_m = c(1,2)
Optimal_Clusters_KMeans(X, max_clusters = tmp_m, criterion = 'distortion_fK', plot_clusters = FALSE)
Optimal_Clusters_KMeans(X, max_clusters = tmp_m, criterion = 'distortion_fK', plot_clusters = FALSE)
testthat::test_that("in case that the max_clusters parameter is not numeric, it returns an error", {
tmp_m = data.frame(1)
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = tmp_m, criterion = 'distortion_fK', plot_clusters = FALSE) )
})
testthat::test_that("in case that the length of the max_clusters parameter is not 1, it returns an error", {
tmp_m = c(1,2)
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = tmp_m, criterion = 'distortion_fK', plot_clusters = FALSE) )
})
testthat::test_that("in case that the max_clusters parameter is less than 1, it returns an error", {
tmp_m = 0
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = tmp_m, criterion = 'distortion_fK', plot_clusters = FALSE) )
})
Optimal_Clusters_KMeans(X, max_clusters = tmp_m, criterion = 'invalid', plot_clusters = FALSE)
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'invalid', plot_clusters = FALSE)
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', num_init = 1, plot_clusters = FALSE)
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', num_init = 0, plot_clusters = FALSE)
testthat::test_that("in case that the num_init parameter is less than 1, it returns an error", {
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', num_init = 0, plot_clusters = FALSE) )
})
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', max_iters = 1, plot_clusters = FALSE)
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', max_iters = 0, plot_clusters = FALSE)
testthat::test_that("in case that the max_iters parameter is less than 1, it returns an error", {
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', max_iters = 0, plot_clusters = FALSE) )
})
Optimal_Clusters_KMeans(X, max_clusters = 5, initializer = 'invalid', plot_clusters = FALSE)
testthat::test_that("if the initializer is not one of c('kmeans++', 'random', 'optimal_init', 'quantile_init'), it returns an error", {
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = 5, initializer = 'invalid', plot_clusters = FALSE) )
})
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', threads = 1, plot_clusters = FALSE)
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', threads = 0, plot_clusters = FALSE)
testthat::test_that("in case that the threads parameter is less than 1, it returns an error", {
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', threads = 0, plot_clusters = FALSE) )
})
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', tol = 0.0, plot_clusters = FALSE)
testthat::test_that("in case that the tol parameter is less than or equal to 0.0, it returns an error", {
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', tol = 0.0, plot_clusters = FALSE) )
})
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', plot_clusters = 'FALSE')
testthat::test_that("in case that the plot_clusters parameter is not logical, it returns an error", {
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', plot_clusters = 'FALSE') )
})
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', verbose = 'FALSE', plot_clusters = FALSE)
testthat::test_that("in case that the verbose parameter is not logical, it returns an error", {
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', verbose = 'FALSE', plot_clusters = FALSE) )
})
Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', tol_optimal_init = 0.0, plot_clusters = FALSE)
testthat::test_that("in case that the tol_optimal_init parameter is less than or equal to 0.0, it returns an error", {
testthat::expect_error( Optimal_Clusters_KMeans(X, max_clusters = 5, criterion = 'distortion_fK', tol_optimal_init = 0.0, plot_clusters = FALSE) )
})
Rcpp::sourceCpp('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/kmeans.cpp')
km = KMeans_rcpp(dat, clusters = 2, num_init = 5, max_iters = 100, initializer = 'optimal_init')
str(km)
km$clusters
l = evaluation_rcpp(X, km$clusters, silhouette = F)
str(l)
l = evaluation_rcpp(X, km$clusters, silhouette = T)
library(gtools)
l = evaluation_rcpp(X, km$clusters, silhouette = T)
str(l)
source('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/clustering_functions.R')
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'distortion_fK', plot_clusters = T)
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'variance_explained', plot_clusters = T)
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'WCSSE', plot_clusters = T)
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'dissimilarity', plot_clusters = T)
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'silhouette', plot_clusters = T)
opt
0/10
source('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/clustering_functions.R')
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'silhouette', plot_clusters = T)
source('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/clustering_functions.R')
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'silhouette', plot_clusters = T)
axis(2, at = seq(0, y_MAX, by = 0.01 ), las = 1, cex.axis = 0.8)
source('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/clustering_functions.R')
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'silhouette', plot_clusters = T)
source('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/clustering_functions.R')
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'silhouette', plot_clusters = T)
opt
source('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/clustering_functions.R')
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'silhouette', plot_clusters = T)
source('~/Desktop/kaggle_gpu/PACKAGES_FUNCTIONS/CLUSTERING/clustering_functions.R')
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'silhouette', plot_clusters = T)
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'dissimilarity', plot_clusters = T)
opt
data("iris")
d = as.matrix(iris[, -ncol(iris)])
opt = Optimal_Clusters_KMeans(d, max_clusters = 10, criterion = 'dissimilarity', plot_clusters = T)
opt = Optimal_Clusters_KMeans(d, max_clusters = 10, criterion = 'silhouette', plot_clusters = T)
data(ionosphere, package = 'KernelKnn')
io = as.matrix(ionosphere[, -c(2, ncol(ionosphere))])
opt = Optimal_Clusters_KMeans(io, max_clusters = 10, criterion = 'silhouette', plot_clusters = T)
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'AIC', plot_clusters = T)
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'BIC', plot_clusters = T)
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'Adjusted_Rsquared', plot_clusters = T)
opt
length(opt)
vec = c('variance_explained', 'WCSSE', 'dissimilarity', 'silhouette', 'AIC', 'BIC', 'Adjusted_Rsquared')
res = rep(NA, length(vec))
nr_clust = 10
count = 1
for (i in vec) {
res =  Optimal_Clusters_KMeans(dat, max_clusters = nr_clust, criterion = 'distortion_fK', plot_clusters = FALSE)
res[count] = (length(res) == nr_clust && class(res) == "k-means clustering")
count = count + 1
}
vec = c('variance_explained', 'WCSSE', 'dissimilarity', 'silhouette', 'AIC', 'BIC', 'Adjusted_Rsquared')
res = rep(NA, length(vec))
nr_clust = 10
count = 1
for (i in vec) {
res =  Optimal_Clusters_KMeans(dat, max_clusters = nr_clust, criterion = i, plot_clusters = FALSE)
res[count] = (length(res) == nr_clust && class(res) == "k-means clustering")
count = count + 1
}
res
(length(res) == nr_clust && class(res) == "k-means clustering")
res
vec = c('variance_explained', 'WCSSE', 'dissimilarity', 'silhouette', 'AIC', 'BIC', 'Adjusted_Rsquared')
out = rep(NA, length(vec))
nr_clust = 10
count = 1
for (i in vec) {
res =  Optimal_Clusters_KMeans(dat, max_clusters = nr_clust, criterion = i, plot_clusters = FALSE)
out[count] = (length(res) == nr_clust && class(res) == "k-means clustering")
count = count + 1
}
sum(out) == length(vec)
out
testthat::test_that("Optimal_Clusters_KMeans returns the correct output for different criteria [except for the 'distortion_fK' criterion,
which doesn't return a vector of numeric values] ", {
vec = c('variance_explained', 'WCSSE', 'dissimilarity', 'silhouette', 'AIC', 'BIC', 'Adjusted_Rsquared')
out = rep(NA, length(vec))
nr_clust = 10
count = 1
for (i in vec) {
res =  Optimal_Clusters_KMeans(dat, max_clusters = nr_clust, criterion = i, plot_clusters = FALSE)
out[count] = (length(res) == nr_clust && class(res) == "k-means clustering")
count = count + 1
}
testthat::expect_true( sum(out) == length(vec) )
})
opt = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = 'distortion_fK', plot_clusters = T)
opt
vec = c('variance_explained', 'WCSSE', 'dissimilarity', 'silhouette', 'AIC', 'BIC', 'distortion_fK', 'Adjusted_Rsquared')
out = rep(NA, length(vec))
nr_clust = 5
count = 1
for (i in vec) {
res =  Optimal_Clusters_KMeans(dat, max_clusters = nr_clust, criterion = i, plot_clusters = FALSE)
out[count] = (length(res) == nr_clust && class(res) == "k-means clustering")
count = count + 1
}
out
testthat::test_that("Optimal_Clusters_KMeans returns the correct output for different criteria [except for the 'distortion_fK' criterion,
which returns the WCSSE ] ", {
vec = c('variance_explained', 'WCSSE', 'dissimilarity', 'silhouette', 'AIC', 'BIC', 'distortion_fK', 'Adjusted_Rsquared')
out = rep(NA, length(vec))
nr_clust = 5
count = 1
for (i in vec) {
res =  Optimal_Clusters_KMeans(dat, max_clusters = nr_clust, criterion = i, plot_clusters = FALSE)
out[count] = (length(res) == nr_clust && class(res) == "k-means clustering")
count = count + 1
}
testthat::expect_true( sum(out) == length(vec) )
})
